\section{Use cases}
We have following use cases:

\subsection{NWCHEM}
The NWChemEx project we will study processes involving transmembrane proteins and zeolite catalysts. The processes of interest require the calculation of free energies and the dynamics of the molecular structures. In addition, to simulate realistic molecular environments, the molecular structures will have at least 100,000 atoms, different regions may be evaluated at different levels of approximation, and the simulations will work with time steps of about one femtosecond.  Hence, to sample enough of the phase space on the order of one million time steps will be evaluated. The simulation requires the evaluation and forces to update the molecular structures. While these calculations are executed, the statistics needed for the free energies are collected. In addition, selected structures along the trajectory will be stored, so that additional properties for those structure can be calculated. These additional properties will facilitate comparing the calculated trajectories against experimental observations.  Hence, the workflow will include a large number of cores calculating the energies and forces and a small number of cores analyzing the results and storing selected time steps for more detailed simulations. 

%\subsubsection{Requirements for the Online Performance Analysis}
A large number of parameters define the total amount of work performence in particular parts of the simulation and varying the amount of work will change the optimal work distribution. Performance characteristics will be recorded in a way that enables comparison against prior simulations to establish the figures of merit of the development. This requires capturing some base characteristics that are always the same. 
For specific performance optimizations, we will capture the performance of specific parts of the code, depending, for example, on the functionality of interest, on the characteristics of the data distribution, or on the granularity of the tensor blocks and associated task sizes. 
%% Dependent on these kinds of characteristics the data collection may be turned on or off. 
The shear volume of the data expected requires the analysis to be performed online.

To extract and analyze interesting events both for performance and scientific results with online analysis, prescriptive provenance will be extracted by Chimbuko.  This provenance extraction is needed to provide the execution metrics used to build training sets, classify features of interest, and select relevant events. 

\subsection{Lattie QCD}
Last year, we used TAU to benchmark a single lattice QCD calculation \url{https://github.com/meifeng/Example-LatticeQCD-With-TAU}.   This sinngle calculation is not representative of the typical production lattice QCD simulations, which are orchestrated in workflows consisting of several complex components, including propagator calculations and contractions. Provenance needs to be extracted and persisted as workflows exhibit complex interdependencies at runtime to enable diagnosis of latencies and bottlenecks. The performances of these calculations are often limited by the data transfer rates. I/O can also be a limiting factor for some algorithms in the lattice QCD workflow. This year, while the lattice QCD code is undergoing constant development, we will use Chimbuko to get a more comprehensive understanding of the performance bottlenecks both to guide our development and to provide feedback to the tool developers.
%% As the code is evolving to adapt to pre-exascale architectures such as Summit, we will target to have the first comprehensive study of the various performance metrics related to lattice QCD simulations at the beginning of FY19. 

\subsection {LAMMPS}
LAMMPS (Large Scale Atomic/Molecular Massively Parallel Simulator) is a widely used molecular dynamics simulation engine for studying materials. Our LAMMPS use case is a workflow composed of three components: (1) the LAMMPS application; (2) the Voro++ analytics engine; and (3) an ADIOS-based parallel data writer (stage\_write). The workflow is configurable, so that LAMMPS can communicate with Voro++ either directly or through stage\_write. The flexibility of the LAMMPS use case can help explore performance tradeoffs on different machines when selecting different communication strategies.

\subsection{Fusion}
Some members of the CODAR team and other ECP projects collaborated to produce an integrate demo using ECP technologies to couple two fusion simulations, one for the core of the plasma and one for the edge, each running on Titan.  ADIOS was used for data exchange at every timestep and TAU captured and aggregated performance data at runtime. The performance data measurement was limited to MPI and ADIOS events and overall application performance.  The performance data was analyzed at runtime, extracting MPI and PMI coordinate information, so that the performance data could be visualized at runtime using the PMI coordinates.  In coupling, we need to analyze aspects of the communication pattern not only between the core and edge codes but also the inter communications of each application to decide optimal placement of coupling processes.  Prescriptive provenance specifies detailed metrics of this communication. 
The performance data extraction was simplified to produce 2D scatterplots of memory consumption and FLOPS for each process to provide a ``dashboard'' for runtime observation.  This simulation could benefit from Chimbuko integration by introducing anomaly detection and richer visualization than is currently provided. 

